{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class MergeLoader(Iterable):\n",
    "    def __init__(self, datasets, batch_size, shuffle):\n",
    "        '''\n",
    "        datasets: list of datasets -- all has to be iterable,\n",
    "            resettable, and same shuffling properties\n",
    "        batch_size: int -- size of the batch\n",
    "        shuffle: bool -- whether to shuffle the merged dataset\n",
    "        '''\n",
    "        self.datasets = datasets\n",
    "        self.num_datasets = len(datasets)\n",
    "        self.bsz = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.buffers = None\n",
    "\n",
    "\n",
    "    def init_buffers(self):\n",
    "        # if this yields an error, your iterator is bad.\n",
    "        if self.shuffle:\n",
    "            # this now represents the buffer itself\n",
    "            self.iterators = [iter(d) for d in self.datasets]\n",
    "            self.buffers = [next(d) for d in self.iterators]\n",
    "            self.exhausted = [False for _ in self.datasets]\n",
    "        else:\n",
    "            self.current_dataset = 0\n",
    "            self.iterator = iter(self.datasets[0])\n",
    "            self.buffers = next(self.iterator)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def unshuffle_pump(self, is_returning=False):\n",
    "        # if is_returning is True, the buffer is being prepared to return\n",
    "        # -> now can raise StopIteration\n",
    "        # make sure the buffer is at least bsz, or the dataset is exhausted\n",
    "        while self.buffers[0].size(0) < self.bsz and self.current_dataset < self.num_datasets:\n",
    "            try:\n",
    "                _data, _label = next(self.iterator)\n",
    "                self.buffers[0] = torch.cat([self.buffers[0], _data])\n",
    "                self.buffers[1] = torch.cat([self.buffers[1], _label])\n",
    "            except StopIteration:\n",
    "                self.current_dataset += 1\n",
    "                if self.current_dataset < self.num_datasets:\n",
    "                    self.iterator = iter(self.datasets[self.current_dataset])\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        # if the buffer is still empty, the dataset is exhausted\n",
    "        if self.buffers[0].size(0) == 0 and is_returning:\n",
    "            self.buffers = None\n",
    "            raise StopIteration\n",
    "        \n",
    "\n",
    "    def shuffle_pump(self, idx, req_size):\n",
    "        # make sure the buffer is at least bsz, or the dataset is exhausted\n",
    "        # if so, update self.exhausted accordingly\n",
    "        while self.buffers[idx][0].size(0) < req_size and not self.exhausted[idx]:\n",
    "            try:\n",
    "                _data, _label = next(self.iterators[idx])\n",
    "                self.buffers[idx][0] = torch.cat([self.buffers[idx][0], _data])\n",
    "                self.buffers[idx][1] = torch.cat([self.buffers[idx][1], _label])\n",
    "            except StopIteration:\n",
    "                self.exhausted[idx] = True\n",
    "        \n",
    "\n",
    "    def __next__(self):\n",
    "        # initialize buffer if necessary\n",
    "        if self.buffers is None:\n",
    "            self.init_buffers()\n",
    "\n",
    "        if self.shuffle:\n",
    "            batch_placeholder = None\n",
    "            count_left = self.bsz\n",
    "\n",
    "            while count_left > 0 and not all(self.exhausted):\n",
    "                for i in range(self.num_datasets):\n",
    "                    # get a random number of data from each dataset\n",
    "                    if i < self.num_datasets - 1:\n",
    "                        idx_count = (torch.randint(i, self.num_datasets, (count_left,)) == i).sum().item()\n",
    "                    else:\n",
    "                        idx_count = count_left\n",
    "\n",
    "                    if idx_count == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    self.shuffle_pump(i, idx_count)\n",
    "                    data_to_add = self.buffers[i][0][:idx_count]\n",
    "                    label_to_add = self.buffers[i][1][:idx_count]\n",
    "                    self.buffers[i] = [self.buffers[i][0][idx_count:], self.buffers[i][1][idx_count:]]\n",
    "\n",
    "                    count_left -= data_to_add.size(0)\n",
    "\n",
    "                    if batch_placeholder is None:\n",
    "                        batch_placeholder = data_to_add, label_to_add\n",
    "                    else:\n",
    "                        batch_placeholder = (\n",
    "                            torch.cat([batch_placeholder[0], data_to_add]),\n",
    "                            torch.cat([batch_placeholder[1], label_to_add])\n",
    "                        )\n",
    "\n",
    "            if count_left == self.bsz:\n",
    "                self.buffers = None\n",
    "                raise StopIteration\n",
    "            else:\n",
    "                # shuffle the batch\n",
    "                idxs = torch.randperm(batch_placeholder[0].size(0))\n",
    "                return batch_placeholder[0][idxs], batch_placeholder[1][idxs]\n",
    "\n",
    "        else:\n",
    "            # if not shuffling, just yield from each dataset sequentially\n",
    "            # the assumption is that either the buffer is long enough for bsz,\n",
    "            # or the dataset is exhausted (after this iteration)\n",
    "            self.unshuffle_pump(is_returning=True)\n",
    "            ret = (self.buffers[0][:self.bsz], self.buffers[1][:self.bsz])\n",
    "            self.buffers = [self.buffers[0][self.bsz:], self.buffers[1][self.bsz:]]\n",
    "            self.unshuffle_pump()\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.utils import set_seed\n",
    "from src.train_utils import AverageMeter\n",
    "\n",
    "\n",
    "def evaluate(trainloader, testloader, configs):\n",
    "    if configs.seed:\n",
    "        set_seed(configs.seed)\n",
    "\n",
    "    if isinstance(trainloader, (list, tuple)):\n",
    "        trainloader = MergeLoader(trainloader, configs.bsz, True)\n",
    "    if isinstance(testloader, (list, tuple)):\n",
    "        testloader = MergeLoader(testloader, configs.bsz, False)\n",
    "\n",
    "    model = configs.model\n",
    "    optimizer = configs.optimizer(model.parameters(), lr=configs.lr, **(configs.optimizer_kwargs if configs.optimizer_kwargs else {}))\n",
    "    scheduler = configs.scheduler(optimizer, **configs.scheduler_kwargs) if configs.scheduler else None\n",
    "    criterion = configs.criterion\n",
    "    device = configs.device if configs.device else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # train the model with trainloader\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    for _ in range(configs.epochs):\n",
    "        avg_loss = AverageMeter()\n",
    "        for data, label in trainloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            avg_loss.update(loss.item(), data.size(0))\n",
    "        train_losses.append(avg_loss.avg)\n",
    "\n",
    "    # evaluate the model with testloader\n",
    "    model.eval()\n",
    "    # keep track of whatever here\n",
    "    acc_meter = AverageMeter()\n",
    "\n",
    "    for data, label in testloader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        output = model(data)\n",
    "        acc = (output.argmax(1) == label).float().mean().item()\n",
    "        acc_meter.update(acc, data.size(0))\n",
    "\n",
    "    # write everything to a file\n",
    "    savedir = Path(configs.save_dir)\n",
    "    savedir = savedir / configs.name / configs.timestamp\n",
    "    savedir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with open(savedir / f'{configs.name}_results.txt', 'w') as f:\n",
    "        f.write(f'{train_losses=}\\n')\n",
    "        f.write(f'acc={acc_meter.avg}\\n')\n",
    "    \n",
    "    # save the model\n",
    "    # delete configs.model\n",
    "    del configs.model\n",
    "\n",
    "    torch.save({\n",
    "            'ckpt': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict() if scheduler else None,\n",
    "            'configs': vars(configs),\n",
    "        },\n",
    "        savedir / f'{configs.name}_model.pth'\n",
    "    )\n",
    "\n",
    "    print(f'[+] {configs.name}')\n",
    "    print(f'    - avg_train_loss={sum(train_losses)/len(train_losses)}')\n",
    "    print(f'    - avg_test_acc={acc_meter.avg}')\n",
    "\n",
    "    return train_losses, acc_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset, copied straight from the acc90 notebook\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a pytorch dataloader for this dataset\n",
    "class HAM10000(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        X = Image.open(self.df['path'][index])\n",
    "        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "\n",
    "data_dir = '/home/ngoc/.cache/kagglehub/datasets/kmader/skin-cancer-mnist-ham10000/versions/2'\n",
    "df_train = pd.read_pickle(data_dir+'/train_data.pkl')\n",
    "df_val = pd.read_pickle(data_dir+'/val_data.pkl')\n",
    "\n",
    "normMean = [0.763033, 0.5456458, 0.5700401]\n",
    "normStd = [0.14092815, 0.15261315, 0.16997056]\n",
    "\n",
    "norm_mean = normMean\n",
    "norm_std = normStd\n",
    "input_size = 64\n",
    "\n",
    "# define the transformation of the train images.\n",
    "train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n",
    "                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
    "                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n",
    "# define the transformation of the val images.\n",
    "val_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\n",
    "                                    transforms.Normalize(norm_mean, norm_std)])\n",
    "\n",
    "\n",
    "# Define the training set using the table train_df and using our defined transitions (train_transform)\n",
    "training_set = HAM10000(df_train, transform=train_transform)\n",
    "train_loader = DataLoader(training_set, batch_size=128, shuffle=True, num_workers=54)\n",
    "# Same for the validation set:\n",
    "validation_set = HAM10000(df_val, transform=train_transform)\n",
    "val_loader = DataLoader(validation_set, batch_size=128, shuffle=False, num_workers=54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from src.train_utils import get_cls_model\n",
    "from argparse import Namespace\n",
    "Namespace.__getattr__ = lambda _1, _2: {}\n",
    "\n",
    "# default configurations\n",
    "model = get_cls_model('resnet', num_classes=10, feature_extract=False, use_pretrained=True).to('cuda')\n",
    "\n",
    "configs = {\n",
    "    'name': 'test',                     # experiment name\n",
    "    'save_dir': './sk-lesion-results',  # base directory to save results\n",
    "    'model': model,\n",
    "    'optimizer': torch.optim.SGD,\n",
    "    'scheduler': None,                  # can change here\n",
    "    'criterion': torch.nn.CrossEntropyLoss(),\n",
    "    'lr': 1e-3,\n",
    "    'bsz': 256,\n",
    "    'epochs': 10,\n",
    "    'optimizer_kwargs': {\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 5e-4,\n",
    "    },\n",
    "    'device': 'cuda',\n",
    "    'seed': 42\n",
    "}\n",
    "configs = Namespace(**configs)\n",
    "configs.timestamp = str(int(time()))\n",
    "\n",
    "# append generated data loaders here -- note that whatever appended has to be iterable!\n",
    "trainloader = [train_loader]\n",
    "testloader = [val_loader]\n",
    "\n",
    "# set_seed is run at the beginning of evaluate\n",
    "evaluate(trainloader, testloader, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
